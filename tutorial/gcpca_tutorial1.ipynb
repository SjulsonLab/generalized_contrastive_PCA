{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "mo.md(\"# How to use the gcPCA toolbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In this marimo notebook I will demonstrate how to use the gcPCA toolbox in python and explain its outputs.\n",
    "\n",
    "For this notebook, we will work with toy data, the same used for Figure 1 on the [paper](https://doi.org/10.1371/journal.pcbi.1012747).\n",
    "\n",
    "To run this notebook locally, you will need the following:\n",
    "\n",
    "python packages:\n",
    "```\n",
    "- marimo\n",
    "- numpy\n",
    "- scipy\n",
    "- generalized_contrastive_PCA\n",
    "```\n",
    "You can find a tutorial_environment.yml in the github tutorial folder to create an environment with all the required python packages. Using conda:\n",
    "\n",
    "```\n",
    "conda env create -f tutorial_environment.yml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "from generalized_contrastive_PCA import gcPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Generate the toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Fitting PCA and gcPCA\n",
    "\n",
    "For PCA we will use the SVD function from numpy. The gcPCA is set up as a class that has to be initialized and fitted. Here I'll use gcPCA version 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### gcPCA outputs\n",
    "\n",
    "PCA returns loadings (the principal components in $V$), the scores of each PC in the samples ($U$) and the variance explained by each component. We organized the output of gcPCA in the same way.\n",
    "\n",
    "- loadings_ : The gcPCA dimensions loadings, or gcPCs. They have size n x p, where n is the number of features and p the number of gcPCs. The gcPCs are ranked based on whether they explain more variance in condition A vs condition B, with the first gcPCs explaining more variance in condition A.\n",
    "- Ra_scores_ : The gcPCs' scores in condition A\n",
    "- Ra_values_ : The l2-norm of gcPCs in condition A, similar to eigenvalue\n",
    "- Rb_scores_ : The gcPCs' scores in condition B\n",
    "- Rb_values_ : The l2-norm of gcPCs in condition B, similar to eigenvalue\n",
    "- objective_values_ : The value of the objective function selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Interpretation and comparison to PCA\n",
    "\n",
    "From the previous plot, you can notice that the gcPCA values do not maximize the data explained within a single dataset (the last two right plots). That is because the method maximizes the data explained in condition A while minimizes in condition B, and vice versa.\n",
    "This maximization of relative variance can be better observed in the lower left panel plot of objective values. Two prominent objective values of gcPCA are positive, reflecting the gcPCs related to condition A. Two prominent gcPCs are negative, reflecting condition B.\n",
    "\n",
    "Let's compare the results of gcPCA and PCA for condition A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "The top PCs do not identify the latent factors we created in the toy data. That's because its variance is very low compared to other latent factors, the top PCs explain about 3-2% of the data's variance. The gcPCA can successfully identify the latent variables we created in the toy data, and the variance they explain is very low because they reflect dimensions of low variance. The variance of only the first 10 dimensions of each method is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Define ancillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     },
     "name": "ancillaries"
    }
   },
   "outputs": [],
   "source": [
    "# Ancillary functions for the tutorial\n",
    "\n",
    "def generate_data(N_samples, N_features):\n",
    "    from scipy.stats import zscore\n",
    "    from scipy.linalg import orth\n",
    "    increased_factor = 2\n",
    "\n",
    "    # ancillary function to generate latent_factors 1 through 4\n",
    "    def generate_latent_factors(N_samples):\n",
    "        # factors that increase in condition A\n",
    "        latent_factor1 = np.random.rand(int(N_samples), 1)\n",
    "        latent_factor2 = np.random.rand(int(N_samples), 1)\n",
    "        idx1 = np.logical_and(np.logical_and(latent_factor1 > 0.3, latent_factor1 < 0.7),\n",
    "                              np.logical_and(latent_factor2 > 0.4, latent_factor2 < 0.6))\n",
    "        latent_factor1[idx1.flatten()] = np.random.rand(sum(idx1)[0], 1) * 0.4\n",
    "        latent_factor2[idx1.flatten()] = np.random.rand(sum(idx1)[0], 1) * 0.4\n",
    "        latent_factor1, latent_factor2 = latent_factor1 - 0.5, latent_factor2 - 0.5\n",
    "        scores_for_color = np.arctan2(latent_factor2, latent_factor1).flatten()\n",
    "        Isort = np.argsort(scores_for_color)\n",
    "        latent_factor1 = latent_factor1[Isort]\n",
    "        latent_factor2 = latent_factor2[Isort]\n",
    "\n",
    "        # factors increased in condition B\n",
    "        temp1 = np.random.rand(int(N_samples))\n",
    "        temp2 = np.random.rand(int(N_samples))\n",
    "        Isort = np.argsort(temp1.flatten())\n",
    "        temp1, temp2 = temp1[Isort], temp2[Isort]\n",
    "        theta = np.deg2rad(45)\n",
    "        latent_factor3 = temp1 * np.cos(theta) + temp2 * -np.sin(theta)\n",
    "        latent_factor4 = temp1 * np.sin(theta) + temp2 * np.cos(theta)\n",
    "\n",
    "        return latent_factor1, latent_factor2, latent_factor3, latent_factor4\n",
    "\n",
    "    latent_factor1, latent_factor2, latent_factor3, latent_factor4 = generate_latent_factors(N_samples)\n",
    "\n",
    "    # normalizing by the l2 norm\n",
    "    latent_factor1 = latent_factor1 / np.linalg.norm(latent_factor1)\n",
    "    latent_factor2 = latent_factor2 / np.linalg.norm(latent_factor2)\n",
    "    latent_factor3 = latent_factor3 / np.linalg.norm(latent_factor3)\n",
    "    latent_factor4 = latent_factor4 / np.linalg.norm(latent_factor4)\n",
    "\n",
    "    # other factors\n",
    "    rest_factors1 = zscore(np.random.randn(N_samples, N_features))\n",
    "    rest_factors1 = rest_factors1 / np.linalg.norm(rest_factors1, axis=0)\n",
    "\n",
    "    # generating data condition A\n",
    "    # eigenvalues\n",
    "    Sa = (np.linspace(0, stop=10, num=N_features)[::-1] + 10 ** -4)\n",
    "    Sa[70] = increased_factor * Sa[70]  # boosting the dimensions' eigenvalue\n",
    "    Sa[71] = increased_factor * Sa[71]  # boosting the dimensions' eigenvalue\n",
    "\n",
    "    # getting orthogonal weights\n",
    "    W = orth(np.random.randn(N_features, N_features)).T\n",
    "\n",
    "    # generating samples from the low variance manifold in condition A\n",
    "    samples1 = np.outer(latent_factor1, Sa[70] * W[70, :])\n",
    "    samples2 = np.outer(latent_factor2, Sa[71] * W[71, :])\n",
    "\n",
    "    # generating samples from the other factors\n",
    "    auxSm = np.repeat(Sa[:70, np.newaxis], N_features, axis=1)\n",
    "    newW = np.multiply(auxSm, W[:70, :])\n",
    "    rest_samples1 = np.dot(rest_factors1[:, :70], newW)\n",
    "\n",
    "    auxSm = np.repeat(Sa[72:, np.newaxis], N_features, axis=1)\n",
    "    newW = np.multiply(auxSm, W[72:, :])\n",
    "    rest_samples2 = np.dot(rest_factors1[:, 72:], newW)\n",
    "\n",
    "    # adding for final data\n",
    "    data_A = samples1 + samples2 + rest_samples1 + rest_samples2\n",
    "\n",
    "    # generating data_B\n",
    "    # eigenvalues\n",
    "    Sb = (np.linspace(0, stop=10, num=N_features)[::-1] + 10 ** -4)\n",
    "    Sb[80] = increased_factor * Sb[80]  # boosting the dimensions' eigenvalue\n",
    "    Sb[81] = increased_factor * Sb[81]  # boosting the dimensions' eigenvalue\n",
    "\n",
    "    # generating samples from the low variance manifold in condition B\n",
    "    samples3 = np.outer(latent_factor3, Sb[80] * W[80, :])\n",
    "    samples4 = np.outer(latent_factor4, Sb[81] * W[81, :])\n",
    "\n",
    "    # other factors\n",
    "    rest_factors1 = zscore(np.random.randn(N_samples, N_features))\n",
    "    rest_factors1 = rest_factors1 / np.linalg.norm(rest_factors1, axis=0)\n",
    "\n",
    "    # generating samples from the other factors\n",
    "    auxSm = np.repeat(Sb[:80, np.newaxis], N_features, axis=1)\n",
    "    newW = np.multiply(auxSm, W[:80, :])\n",
    "    rest_samples1 = np.dot(rest_factors1[:, :80], newW)\n",
    "\n",
    "    auxSm = np.repeat(Sb[82:, np.newaxis], N_features, axis=1)\n",
    "    newW = np.multiply(auxSm, W[82:, :])\n",
    "    rest_samples2 = np.dot(rest_factors1[:, 82:], newW)\n",
    "\n",
    "    # adding for final data\n",
    "    data_B = samples3 + samples4 + rest_samples1 + rest_samples2\n",
    "\n",
    "    return data_A, data_B, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy data\n",
    "\n",
    "# this will create datasets from condition A (data_A) and B (data_B), and return their original weights (W)\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "data_A, data_B, W = generate_data(n_samples,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "U,S,V = np.linalg.svd(data_A,full_matrices=False)\n",
    "\n",
    "# creating gcPCA model\n",
    "gcPCA_model = gcPCA(method='v4',normalize_flag=False)\n",
    "\n",
    "# fitting gcPCA\n",
    "gcPCA_model.fit(data_A,data_B)\n",
    "\n",
    "# gcPCA outputs\n",
    "print('gcPCA_model.loadings_ shape: '+str(gcPCA_model.loadings_.shape))\n",
    "print('gcPCA_model.Ra_values_ shape: '+str(gcPCA_model.Ra_values_.shape))\n",
    "print('gcPCA_model.Ra_scores_ shape: '+str(gcPCA_model.Ra_scores_.shape))\n",
    "print('gcPCA_model.Rb_values_ shape: '+str(gcPCA_model.Rb_values_.shape))\n",
    "print('gcPCA_model.Rb_scores_ shape: '+str(gcPCA_model.Rb_scores_.shape))\n",
    "print('gcPCA_model.objective_values_ shape: '+str(gcPCA_model.objective_values_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# picking mapping to color the latent variables\n",
    "hsv_cmap = mpl.colormaps.get_cmap('hsv')\n",
    "spec_cmap = mpl.colormaps.get_cmap('PRGn')\n",
    "\n",
    "# making the color to map to the latent variables\n",
    "temp_t = np.linspace(0,1,n_samples)\n",
    "hsv_col = hsv_cmap(temp_t)\n",
    "spec_col = spec_cmap(temp_t)\n",
    "\n",
    "# Create a figure with constrained layout\n",
    "fig = plt.figure(figsize=(8, 6), constrained_layout=True)\n",
    "gs = gridspec.GridSpec(2, 3, figure=fig)\n",
    "\n",
    "# Helper to force square axes\n",
    "def make_square(ax):\n",
    "    ax.set_box_aspect(1)  # This ensures the subplot is a square (1:1 height:width)\n",
    "\n",
    "# plot loadings\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.stem(gcPCA_model.loadings_[:,0])\n",
    "ax1.set_xlabel('Features')\n",
    "ax1.set_ylabel('Loadings')\n",
    "ax1.set_title('Loadings gcPC1')\n",
    "make_square(ax1)\n",
    "\n",
    "# plot condition A scores and values\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.scatter(gcPCA_model.Ra_scores_[:, 0], gcPCA_model.Ra_scores_[:, 1],c=hsv_col,s=4)\n",
    "ax2.set_xlabel('gcPC1')\n",
    "ax2.set_ylabel('gcPC2')\n",
    "ax2.set_title('scores condition A')\n",
    "make_square(ax2)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.plot(gcPCA_model.Ra_values_)\n",
    "ax3.set_xlabel('gcPCs')\n",
    "ax3.set_ylabel('gcPCA `eigenvalue`')\n",
    "ax3.set_title('condition A')\n",
    "make_square(ax3)\n",
    "\n",
    "# plot objective values\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.plot(gcPCA_model.objective_values_)\n",
    "ax4.set_xlabel('gcPCs')\n",
    "ax4.set_ylabel('Objective values')\n",
    "ax4.set_title('(A-B)/(A+B)')\n",
    "make_square(ax4)\n",
    "\n",
    "# plot condition B scores and values\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.scatter(gcPCA_model.Rb_scores_[:, -1], gcPCA_model.Rb_scores_[:, -2],c=spec_col,s=4)\n",
    "ax5.set_xlabel('gcPC(last)')\n",
    "ax5.set_ylabel('gcPC(last-1)')\n",
    "ax5.set_title(' scores condition B')\n",
    "make_square(ax5)\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.plot(gcPCA_model.Rb_values_)\n",
    "ax6.set_xlabel('gcPCs')\n",
    "ax6.set_ylabel('gcPCA `eigenvalue`')\n",
    "ax6.set_title('condition B ')\n",
    "make_square(ax6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "# plotting the scores of PCA and gcPCA, and their variance explained\n",
    "# Create a figure with constrained layout\n",
    "fig2 = plt.figure(figsize=(5, 5), constrained_layout=True)\n",
    "gs2 = gridspec.GridSpec(2, 2, figure=fig2)\n",
    "\n",
    "# plot scores of PCA\n",
    "ax7 = fig2.add_subplot(gs2[0, 0])\n",
    "ax7.scatter(U[:,0],U[:,1],c=hsv_col,s=4)\n",
    "ax7.set_xlabel('PC1')\n",
    "ax7.set_ylabel('PC2')\n",
    "ax7.set_title('PCA scores')\n",
    "make_square(ax7)\n",
    "\n",
    "# plot condition A scores\n",
    "ax8 = fig2.add_subplot(gs2[1, 0])\n",
    "ax8.scatter(gcPCA_model.Ra_scores_[:, 0], gcPCA_model.Ra_scores_[:, 1],c=hsv_col,s=4)\n",
    "ax8.set_xlabel('gcPC1')\n",
    "ax8.set_ylabel('gcPC2')\n",
    "ax8.set_title('gcPCA scores')\n",
    "make_square(ax8)\n",
    "\n",
    "# plot scores of PCA\n",
    "total_var_condA = np.var(data_A,axis=0).sum()\n",
    "conditionA_var_PCA = 100*np.var(data_A@V.T,axis=0)[:10] / total_var_condA # condition A variance on the first 10 PCs\n",
    "ax9 = fig2.add_subplot(gs2[0, 1])\n",
    "ax9.plot(conditionA_var_PCA)\n",
    "ax9.set_xlabel('PCs')\n",
    "ax9.set_ylabel('Variance (%)')\n",
    "ax9.set_title('PCA')\n",
    "make_square(ax9)\n",
    "\n",
    "# plot objective values\n",
    "ax10 = fig2.add_subplot(gs2[1, 1])\n",
    "conditionA_var_gcPCA = 100*np.var(data_A@gcPCA_model.loadings_,axis=0)[:10] / total_var_condA\n",
    "ax10.plot(conditionA_var_gcPCA)\n",
    "ax10.set_xlabel('gcPCs')\n",
    "ax10.set_ylabel('Variance (%)')\n",
    "ax10.set_title('gcPCA')\n",
    "make_square(ax10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
