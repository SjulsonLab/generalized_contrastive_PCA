{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo\n",
    "mo.md(\"# How to use sparse gcPCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In this tutorial, I will show you how to use the sparse solution of the gcPCA class with a single-cell RNA sequencing dataset.\n",
    "\n",
    "First, let’s briefly review what a sparse solution is.\n",
    "**If you're already familiar with methods with sparse solutions, feel free to skip this section.**\n",
    "\n",
    "##### **What does a sparse solution to a method do?**\n",
    "\n",
    "Sparse solutions are commonly used for **feature selection** and **model interpretability**. The idea is simple: we want to identify which features are most relevant to what the model has learned from the data.\n",
    "\n",
    "When performing regression or classification, we often want to understand how the model makes its predictions. One way to do this is by inspecting the model’s weights (also called coefficients or loadings) to see which features have the largest impact.\n",
    "\n",
    "However, this becomes difficult when dealing with datasets that have a **large number of features**, which is the case in biomedical data. To address this, a common solution is to add a penalty to the model that discourages large weights unless they are essential. This is the principle behind lasso and ridge regression.\n",
    "\n",
    "A regular linear regression minimizes the residual sum of squares:\n",
    "\n",
    "$\\min_\\beta ||Y - X\\beta||^2$\n",
    "\n",
    "It finds the $\\beta$ coefficients that minimize the difference between the predictions $X\\beta$ and the observed data $Y$.\n",
    "\n",
    "In contrast, lasso regression adds a penalty on the sum of the absolute values of the coefficients (L1 norm), encouraging some coefficients to be exactly zero:\n",
    "\n",
    "$\\min_\\beta ||Y - X\\beta||^2 + \\lambda \\sum |\\beta_j|$\n",
    "\n",
    "\n",
    "Here, $\\lambda$ is a hyperparameter that controls the strength of the penalty:\n",
    "\n",
    "    - Higher $\\lambda$ values increase the penalty, forcing more coefficients to zero.\n",
    "\n",
    "    - Lower $\\lambda$ values allow more features to be retained.\n",
    "\n",
    "After you select an optimal lambda, only a subset of features have non-zero weights, making the model more interpretable.\n",
    "\n",
    "\n",
    "##### **Sparsity in gcPCA**\n",
    "\n",
    "The sparse version of gcPCA builds on existing sparse PCA methods by incorporating an elastic net penalty, which combines both L1 (lasso) and L2 (ridge) regularization:\n",
    "\n",
    "    - L1 (lasso) promotes sparsity by driving some coefficients to zero.\n",
    "\n",
    "    - L2 (ridge) helps stabilize the solution when the data is ill-conditioned (e.g., when features are highly correlated).\n",
    "\n",
    "In our implementation, you only need to control the L1 penalty (lambda), while the L2 penalty (kappa) is set automatically. However, we leave it accessible in case you need to adjust it for your specific use case.\n",
    "\n",
    "The objective function minimized in sparse gcPCA is:\n",
    "\n",
    "$\\hat{\\boldsymbol\\beta}_j = \\arg\\min_{\\boldsymbol\\beta_j} \\left\\| \\mathbf{\\Theta}^{1/2} \\mathbf{y}_j - \\mathbf{\\Theta}^{1/2} \\boldsymbol\\beta_j \\right\\|^2 + \\kappa \\left\\| \\mathbf{J} \\mathbf{M}^{-1} \\boldsymbol\\beta_j \\right\\|^2 + \\lambda \\left\\| \\mathbf{J} \\mathbf{M}^{-1} \\boldsymbol\\beta_j \\right\\|_1$\n",
    "\n",
    "Where:\n",
    "\n",
    "    - $\\lambda$ controls the L1 penalty\n",
    "\n",
    "    - $\\kappa$ controls the L2 penalty\n",
    "\n",
    "    - $\\Theta$, $J$, and $M$ are matrices defined in the gcPCA framework (see full method documentation for details).\n",
    "\n",
    "To obtain different sparse solutions, simply vary the value of $\\lambda$. A higher $\\lambda$ will lead to more sparsity (fewer features retained).\n",
    "\n",
    "> **_IMPORTANT:_** You need to verify the penalty is not distorting your original results, which could impair interpretability\n",
    "\n",
    "Let's start the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from generalized_contrastive_PCA import sparse_gcPCA\n",
    "# for plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Ancillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data(data_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Load gene expression and metadata, and prepare data matrices for sparse gcPCA.\n",
    "\n",
    "    Returns:\n",
    "        gaba_X (np.ndarray): Gene expression matrix for GABAergic cells (samples x genes)\n",
    "        glut_X (np.ndarray): Gene expression matrix for Glutamatergic cells (samples x genes)\n",
    "        gaba_subclasses (List[str]): Subclass annotations for GABAergic cells\n",
    "        glut_subclasses (List[str]): Subclass annotations for Glutamatergic cells\n",
    "    \"\"\"\n",
    "    # Load data and metadata\n",
    "    data_df = pd.read_csv(data_path, index_col=0)  # genes x samples\n",
    "    data_df = data_df.transpose()  # samples x genes\n",
    "\n",
    "    # Remove genes expressed in less than 100 cells\n",
    "    data_df = filter_genes_by_min_cells(data_df,min_cells=100)\n",
    "\n",
    "    # Select the top 1000 most variable genes\n",
    "    data_df = select_highly_variable_genes(data_df,top_n=1000)\n",
    "\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Filter metadata to only GABAergic and Glutamatergic\n",
    "    metadata_df = metadata_df[metadata_df['class'].isin(['GABAergic', 'Glutamatergic'])]\n",
    "\n",
    "    # Filter metadata to samples present in data\n",
    "    metadata_df = metadata_df[metadata_df['sample_name'].isin(data_df.index)]\n",
    "\n",
    "    # Set sample_name as index to align with data_df\n",
    "    metadata_df = metadata_df.set_index('sample_name')\n",
    "\n",
    "    # Subset GABAergic and Glutamatergic samples\n",
    "    gaba_samples = metadata_df[metadata_df['class'] == 'GABAergic'].index\n",
    "    glut_samples = metadata_df[metadata_df['class'] == 'Glutamatergic'].index\n",
    "\n",
    "    # Get matrices (samples x genes)\n",
    "    gaba_X = data_df.loc[gaba_samples].values\n",
    "    glut_X = data_df.loc[glut_samples].values\n",
    "\n",
    "    # Get subclass labels in same order\n",
    "    gaba_subclasses = metadata_df.loc[gaba_samples]['subclass'].values.tolist()\n",
    "    glut_subclasses = metadata_df.loc[glut_samples]['subclass'].values.tolist()\n",
    "\n",
    "    # Log transforming the data\n",
    "    gaba_X = np.log(gaba_X+1)\n",
    "    glut_X = np.log(glut_X+1)\n",
    "\n",
    "    # Centering the data\n",
    "    gaba_X = gaba_X - gaba_X.mean(axis=0)\n",
    "    glut_X = glut_X - glut_X.mean(axis=0)\n",
    "\n",
    "    return gaba_X, glut_X, gaba_subclasses, glut_subclasses\n",
    "\n",
    "\n",
    "\n",
    "def plot_gcPCA_fits(model, subclasses, title_prefix=\"gcPCA\", figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Plot 2D gcPCA embeddings and stem plots of top 100 gene loadings for each lambda,\n",
    "    including the original projection. The first embedding plot includes a legend for the subclasses.\n",
    "    The y-axis limits for the stem plots are fixed based on the original gcPCA loadings,\n",
    "    highlighting the shrinking effect of lasso.\n",
    "\n",
    "    Args:\n",
    "        model: A fitted sparse_gcPCA object.\n",
    "        subclasses: List of subclass labels corresponding to the samples.\n",
    "        title_prefix: Title prefix for each subplot.\n",
    "        figsize: Size of each row (width, height).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "\n",
    "    # Combine original and penalized embeddings and loadings\n",
    "    embeddings = [model.original_gcPCA.Ra_scores_] + model.Ra_scores_\n",
    "    sparse_loadings = [model.original_gcPCA.loadings_] + model.sparse_loadings_\n",
    "    lambda_labels = [\"original\"] + [f\"λ={lam}\" for lam in model.lasso_penalty]\n",
    "    num_plots = len(embeddings)\n",
    "\n",
    "    # Get unique subclasses and assign colors\n",
    "    unique_subclasses = sorted(set(subclasses))\n",
    "    colors = cm.tab20(np.linspace(0, 1, len(unique_subclasses)))\n",
    "    subclass_to_color = dict(zip(unique_subclasses, colors))\n",
    "    subclass_colors = [subclass_to_color[sub] for sub in subclasses]\n",
    "\n",
    "    # Using the original gcPCA loadings, identify the top 200 genes for both PCs.\n",
    "    orig_loadings = model.original_gcPCA.loadings_\n",
    "    top1_idx = np.argsort(-(orig_loadings[:, 0]))[:200]\n",
    "    top2_idx = np.argsort(-(orig_loadings[:, 1]))[:200]\n",
    "\n",
    "    # Determine fixed y-axis limits based on original loadings for the top genes.\n",
    "    max_pc1 = np.max(np.abs(orig_loadings[top1_idx, 0]))\n",
    "    max_pc2 = np.max(np.abs(orig_loadings[top2_idx, 2]))\n",
    "    y_min_pc1 = -1*max_pc1\n",
    "    y_max_pc1 = max_pc1\n",
    "    y_min_pc2 = -1*max_pc2\n",
    "    y_max_pc2 = max_pc2\n",
    "\n",
    "    # Create subplots: each row corresponds to one version (original or lasso-penalized)\n",
    "    fig, axes = plt.subplots(num_plots, 3, figsize=(figsize[0], figsize[1] * num_plots), squeeze=False)\n",
    "\n",
    "    # Pre-create legend handles so we can add the legend on the first plot\n",
    "    legend_handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                                  label=sub, markerfacecolor=color, markersize=6)\n",
    "                      for sub, color in subclass_to_color.items()]\n",
    "\n",
    "    for i, (Z, label, L) in enumerate(zip(embeddings, lambda_labels, sparse_loadings)):\n",
    "        # Embedding plot\n",
    "        ax0 = axes[i, 0]\n",
    "        ax0.scatter(Z[:, 0], Z[:, 1], c=subclass_colors, s=20, alpha=0.7)\n",
    "        ax0.set_title(f\"{title_prefix} {label}\")\n",
    "        ax0.set_xlabel(\"Component 1\")\n",
    "        ax0.set_ylabel(\"Component 2\")\n",
    "        ax0.grid(True)\n",
    "        if i == 0:\n",
    "            # Highlight the first plot and add the subclass legend.\n",
    "            for spine in ax0.spines.values():\n",
    "                spine.set_edgecolor('red')\n",
    "                spine.set_linewidth(2)\n",
    "            ax0.legend(handles=legend_handles, title=\"Subclass\", loc='best')\n",
    "\n",
    "        # Stem plot for PC1 loadings\n",
    "        ax1 = axes[i, 1]\n",
    "        pc1_vals = L[top1_idx, 0]\n",
    "        markerline, stemlines, baseline = ax1.stem(pc1_vals)\n",
    "        plt.setp(markerline, markersize=4)\n",
    "        ax1.set_title(\"Top 200 gcPC1 loadings\")\n",
    "        ax1.set_ylabel(\"Loading\")\n",
    "        ax1.set_xticks(range(200))\n",
    "        # Remove gene names for clarity.\n",
    "        ax1.set_xticklabels([])\n",
    "        # Set the fixed y-axis limits based on the original loadings.\n",
    "        ax1.set_ylim(y_min_pc1, y_max_pc1)\n",
    "\n",
    "        # Stem plot for PC2 loadings\n",
    "        ax2 = axes[i, 2]\n",
    "        pc2_vals = L[top2_idx, 1]\n",
    "        markerline, stemlines, baseline = ax2.stem(pc2_vals)\n",
    "        plt.setp(markerline, markersize=4)\n",
    "        ax2.set_title(\"Top 200 gcPC2 loadings\")\n",
    "        ax2.set_ylabel(\"Loading\")\n",
    "        ax2.set_xticks(range(200))\n",
    "        ax2.set_xticklabels([])\n",
    "        # Set the fixed y-axis limits based on the original loadings.\n",
    "        ax2.set_ylim(y_min_pc2, y_max_pc2)\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def filter_genes_by_min_cells(data_df, min_cells=100):\n",
    "    expressed_counts = (data_df > 0).sum(axis=0)\n",
    "    selected = expressed_counts >= min_cells\n",
    "    filtered_df = data_df.loc[:, selected]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def select_highly_variable_genes(data_df, top_n=1000):\n",
    "    means = data_df.mean(axis=0)\n",
    "    variances = data_df.var(axis=0)\n",
    "    dispersion = variances / means\n",
    "    top_genes = dispersion.nlargest(top_n).index\n",
    "    filtered_df = data_df[top_genes]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Loading and preparing data\n",
    "\n",
    "The data used here is a single-cell RNA sequencing of VISp made available by the Allen Institute for Brain Science, you can download it [**here**](https://portal.brain-map.org/atlases-and-data/rnaseq/mouse-v1-and-alm-smart-seq).\n",
    "\n",
    "I am going to contrast the GABAergic cells (condition A) and Glutamatergic cells (condition B) transcriptomics profiles using gcPCA.\n",
    "\n",
    "> NOTE:\n",
    "If you are running this notebook locally, change the data_path and metadata_path variables to the path where the data is stored on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of data - Update to your own if running this notebook locally!\n",
    "data_path = '/home/eliezyer/Documents/mouse_VISp_gene_expression_matrices_2018-06-14/mouse_VISp_2018-06-14_exon-matrix.csv'\n",
    "metadata_path = '/home/eliezyer/Documents/mouse_VISp_gene_expression_matrices_2018-06-14/mouse_VISp_2018-06-14_samples-columns.csv'\n",
    "\n",
    "gaba_X, glut_X, gaba_subclasses, glut_subclasses = load_and_prepare_data(data_path = data_path, metadata_path = metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Fitting the sparse gcPCA model\n",
    "\n",
    "The sparse gcPCA model can be run on a _subset of dimensions_ rather than the whole space of all gcPCs. This is recommended because it can speed up the processing time. In this tutorial I picked `number_of_dimensions = 2` to fit the model on only two dimensions, make sure you select the dimensionality appropriate for your case.\n",
    "\n",
    "I'm going to use 4 different $\\lambda$ values here. The parameter to control the lasso penalty parameter in the model (controlling the $\\lambda$ parameter discussed in the introduction) is `lasso_penalty`. For the ridge penalty parameter ($\\kappa$), you have to declare `ridge_penalty` in the model initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting gcPCA\n",
    "\n",
    "#parameters\n",
    "lambdas_array = [1e-2,1e-1,1,3] # array of different lambdas to fit\n",
    "number_of_dimensions = 2\n",
    "\n",
    "#initialize the model\n",
    "sparse_gcPCA_model = sparse_gcPCA(method='v4',lasso_penalty=lambdas_array, Nsparse=number_of_dimensions,normalize_flag=True)\n",
    "\n",
    "# fit\n",
    "sparse_gcPCA_model.fit(gaba_X,glut_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "### Visualizing the results\n",
    "\n",
    "Below you are going to find a five-row and three-column figure.\n",
    "\n",
    "In the first row, I show you the plot of the original gcPCA. The first column presents the scores of the gCPCA. You can see the subclasses of interneurons are separated in the top gcPCs, with the first gcPC separating neurons that are Medial Ganglionic Ence (MGE) or Caudal Ganglionic Ence (CGE) derived. The second gcPC further separates the subclasses. The second and third columns show the gcPC1-2 top 200 loadings.\n",
    "\n",
    "In the following rows, I plotted the same results but for the sparse gcPCA of varying lasso penalty ($\\lambda$). The more I increase the lambda, the more loadings are shrunk to zero, making the model easier to interpret. On the scores plot in the first column, you can see that $\\lambda=1$ value used started distorting the clusters, making them closer together. However, their original structure and separability are still maintained, and the model might be used for further analysis/investigation. Increasing the $\\lambda$ further can distort the values in a way that the clusters are overlapping, making the model useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results\n",
    "plot_gcPCA_fits(sparse_gcPCA_model, gaba_subclasses)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
